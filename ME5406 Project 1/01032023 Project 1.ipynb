{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with:\n",
    "- Monte Carlo Simulation Algorithm\n",
    "- SARSA Algorithm\n",
    "- Q Learning Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment and Mission "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The goal is for the agent to learn how to navigate the state space to reach the end goal of retrieving the frisbee*\n",
    "<br></br>\n",
    "\n",
    "<U>**Within Action Space, the following actions are defined:**</U>\n",
    "\n",
    "**'L':** Move left\n",
    "\n",
    "**'D':** Move down\n",
    "\n",
    "**'R':** Move right\n",
    "\n",
    "**'U':** Move up\n",
    "\n",
    "*If agent attempts to leave the grid, when at the edges, program would set the new state as the old state. Basically it will not move\n",
    "<br></br>\n",
    "\n",
    "<U>**Map**:</U>\n",
    "    \n",
    "    S  .  .  .\n",
    "    \n",
    "    .  H  .  H\n",
    "    \n",
    "    .  .  .  H\n",
    "    \n",
    "    H  .  .  E\n",
    "<br></br>\n",
    "<U>**Rewards**:</U>\n",
    "\n",
    "Reach goal: +1\n",
    "\n",
    "Reach hole: -1\n",
    "\n",
    "Traversing frozen surface: 0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Environment\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import statistics as st\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Grid Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Grid Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "\n",
    "    # Takes in variables of rows, cols and start state of agent\n",
    "    def __init__(self, rows, cols, start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "    \n",
    "    # Fucntion that allows user to set rewards and actions allowed at given states\n",
    "    def set(self, rewards, actions):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "    # Function that allows user to set state of agent\n",
    "    def set_state(self,s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "    \n",
    "    # Function that fetches current state of agent\n",
    "    def current_state(self):\n",
    "        return(self.i, self.j)\n",
    "    \n",
    "    # Function that checks if agent is in a terminal state (if current state of agent is in a terminal state: hole / goal state, then function returns True)\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    # Function that fetches the possible actions the agent can take at a given state s\n",
    "    def possible_actions(self, s):\n",
    "        return self.actions[s]\n",
    "    \n",
    "    # Moves the agent in the state space based on the action taken by the agent\n",
    "    def move(self, action):\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "    \n",
    "    # Gets reward of the current state of agent\n",
    "    def get_rewards(self):\n",
    "        reward = self.rewards.get((self.i, self.j), 0)\n",
    "        return reward\n",
    "    \n",
    "    # Undo move of agent (Function isn't used but put in place if needed)\n",
    "    def undo_move(self, action):\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        assert(self.current_state() in self.all_states())\n",
    "    \n",
    "            \n",
    "    # To reset agent to be at starting state - (0, 0) in our specific example\n",
    "    def reset(self):\n",
    "        self.set_state((0,0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Grid Environment Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_grid(rewards, actions, rows, cols, start_state):\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # S means start position\n",
    "    # E means the end states\n",
    "\n",
    "        # S  .  .  .\n",
    "        # .  H  .  H\n",
    "        # .  .  .  H\n",
    "        # H  .  .  E\n",
    "\n",
    "    g = Grid(rows, cols, start_state) #(rows, cols, start_state)\n",
    "    g.set(rewards, actions)\n",
    "    return g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Characteristics\n",
    "# no. of rows & cols of grid\n",
    "no_of_rows = 4\n",
    "no_of_cols = 4\n",
    "\n",
    "# Full action space\n",
    "action_space = ('D', 'U', 'L', 'R')\n",
    "\n",
    "# Assigned start state\n",
    "start_state = (0, 0)\n",
    "\n",
    "# Define rewards at specific states (punishment yields negative rewards)\n",
    "# rewards at given states (in dictionary form)\n",
    "rewards = {(1, 1): -1, # hole\n",
    "           (1, 3): -1, # hole\n",
    "           (2, 3): -1, # hole\n",
    "           (3, 0): -1, # hole\n",
    "           (3, 3): 1} # frisbee\n",
    "\n",
    "# Define legal (possible) actions at each state\n",
    "# States that depict terminal state (hole / end goal) are commented because this will tie in with the .is_terminal() function under class Grid\n",
    "actions = {\n",
    "        (0, 0): ('D', 'R'), # Start_state\n",
    "        (0, 1): ('D', 'R', 'L'), \n",
    "        (0, 2): ('D', 'R', 'L'),\n",
    "        (0, 3): ('D', 'L'),\n",
    "        (1, 0): ('D', 'R', 'U'),\n",
    "        #(1, 1): ('D', 'R', 'L', 'U'), #Hole\n",
    "        (1, 2): ('D', 'R', 'L', 'U'),\n",
    "        #(1, 3): ('D', 'U', 'L'), #Hole\n",
    "        (2, 0): ('D', 'U', 'R'),\n",
    "        (2, 1): ('D', 'R', 'L', 'U'),\n",
    "        (2, 2): ('D', 'R', 'L', 'U'),\n",
    "        #(2, 3): ('D', 'U', 'L'), #Hole\n",
    "        #(3, 0): ('U', 'R', ), #Hole\n",
    "        (3, 1): ('U', 'R', 'L'),\n",
    "        (3, 2): ('U', 'R', 'L'),\n",
    "        #(3, 3): (), #End-State (frisbee)\n",
    "}\n",
    "\n",
    "\n",
    "# Create 4x4 Grid environment\n",
    "env1 = standard_grid(rewards, actions, no_of_rows, no_of_cols, start_state) \n",
    "# Set rewards and actions of environment\n",
    "env1.set(rewards, actions)\n",
    "# Reset environment to start state defined as (0,0) in .reset() function\n",
    "env1.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --- Function testing ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "Original State: (0, 0), After taking action: (1, 0)\n",
      "State before: (1, 0), State After taking action 'R': (1, 1)\n",
      "Reached terminal state (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test if .is_terminal() function works\n",
    "    # Terminal States: 1,1  1,3  2,3  3,0  3,3\n",
    "print(env1.is_terminal((2, 0)))\n",
    "print(env1.is_terminal((3, 0)))\n",
    "\n",
    "\n",
    "# Test .move()\n",
    "env1.reset()\n",
    "state_before = env1.current_state()\n",
    "action = env1.move('D')\n",
    "state_after = env1.current_state()\n",
    "print('Original State: {}, After taking action: {}'.format(state_before, state_after))\n",
    "\n",
    "\n",
    "# Test loop to stop moving when environment reaches terminal state\n",
    "while env1.is_terminal(env1.current_state()) == False:\n",
    "    a = action_space[(random.randint(0, (len(action_space)-1)))]\n",
    "    state_b = env1.current_state()\n",
    "    env1.move(a)\n",
    "    state_a = env1.current_state()\n",
    "    \n",
    "    print(\"State before: {}, State After taking action '{}': {}\".format(state_b, a, state_a))\n",
    "\n",
    "else:\n",
    "    print('Reached terminal state {}'.format(env1.current_state()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q table, Returns table and Policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q table Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q table is built as a dataframe for easier referencing: there were problems with referencing when building a multi nested dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qtable(no_of_rows, no_of_cols, action_space):\n",
    "    # Creates Q table as a nested dictionary\n",
    "    Q = {}\n",
    "    for i in range(no_of_rows):\n",
    "        for j in range(no_of_cols):\n",
    "            Q[(str(i) + str(j))] = 0\n",
    "    \n",
    "    action_space_dic = {}\n",
    "    for item in action_space:\n",
    "        action_space_dic[item] = 0\n",
    "        \n",
    "\n",
    "    for k, v in Q.items():\n",
    "        Q[k] = action_space_dic\n",
    "    \n",
    "    # Converts Q table into a dataframe\n",
    "    Q = pd.DataFrame(data = Q)\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns table Function\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Returns table is built as a dataframe for easier referencing: there were problems with referencing when building a multi nested dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_returnstable(no_of_rows, no_of_cols, action_space):\n",
    "    # Creates Returns table as a nested dictionary\n",
    "    returns = {}\n",
    "    for i in range(no_of_rows):\n",
    "        for j in range(no_of_cols):\n",
    "            returns[(str(i) + str(j))] = 0\n",
    "    \n",
    "    action_space_dic = {}\n",
    "    for item in action_space:\n",
    "        action_space_dic[item] = []\n",
    "        \n",
    "\n",
    "    for k, v in returns.items():\n",
    "        returns[k] = action_space_dic\n",
    "    \n",
    "    # Converts Returns table into a dataframe\n",
    "    returns = pd.DataFrame(data = returns)\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an action for the agent to take. Each action has a minimum probability of (epsilon / no. of actions) of being selected\n",
    "# Optimal action has a higher probability of being selected\n",
    "def epsilon_soft(Qtable, env, epsilon, currentstate):\n",
    "\n",
    "    prob = epsilon # sum of minimum prob of selecting all actions in action space\n",
    "\n",
    "    # Set a random probability to determine which actions are being selected\n",
    "    random_prob = random.random()\n",
    "    best_actions = []\n",
    "    valid_q_values = []\n",
    "\n",
    "    # Finding max q value at the specific state for legal actions\n",
    "    state_f = str(currentstate[0]) + str(currentstate[1]) # Formatted state\n",
    "    for legal_action in env.actions[currentstate]:\n",
    "        valid_q_values.append(Qtable.at[legal_action, state_f])\n",
    "        max_q_value = max(valid_q_values)\n",
    "\n",
    "    # Appending max q value of legal actions to best_actions list\n",
    "    for item in Qtable[Qtable[state_f] == max_q_value].index.values:\n",
    "        if item in env.actions[currentstate]:\n",
    "            best_actions.append(item)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # When random_prob =< sum of min prob of all actions, randomly select action\n",
    "    if random_prob <= prob:\n",
    "        # Select random action from legal actions\n",
    "        valid_actions = env.actions[currentstate]\n",
    "        action = valid_actions[(random.randint(0, (len(valid_actions)-1)))]\n",
    "        return action\n",
    "            \n",
    "    # If random_prob > prob, then select legal action with highest q value\n",
    "        # Other problems that this code solves:\n",
    "            # 1. When more than 1 action has the same q value - select the action randomly\n",
    "    else:\n",
    "        action = best_actions[random.randint(0, len(best_actions)-1)]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-visit Monte Carlo Without Exploring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Algorithm Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class monte_carlo_sim:\n",
    "    '''\n",
    "    ** Please remember to reset Q and Returns table after simulation\n",
    "\n",
    "        Functions:\n",
    "        # Fetches Qtable ->                            .fetchQtable()    \n",
    "        # Fetches Returns Table ->                     .fetchReturnstable() \n",
    "        # Run simulation ->                            .simulate(no_of_episodes, epsilon, gamma)\n",
    "        # Resets Qtable and Returns Table ->           .resettables()      \n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # Create Q and Returns table on creating class\n",
    "        self.Qtable_monte = create_qtable(self.env.rows, self.env.cols, action_space)\n",
    "        self.Returns = create_returnstable(self.env.rows, self.env.cols, action_space)\n",
    "\n",
    "    # Returns Qtable\n",
    "    def fetchQtable(self):\n",
    "        return self.Qtable_monte\n",
    "    \n",
    "    # Returns Returns table\n",
    "    def fetchReturnstable(self):\n",
    "        return self.Returns\n",
    "    \n",
    "    # Reset Q and Returns table by creating empty tables\n",
    "    def resettables(self):\n",
    "        self.Qtable_monte = createQtable()\n",
    "        self.Returns = createReturnstable()\n",
    "\n",
    "    # Run monte carlo simulation\n",
    "    def simulate(self, no_of_episodes, epsilon, gamma):\n",
    "        for i in range(no_of_episodes):\n",
    "            episode = []\n",
    "            G = 0\n",
    "\n",
    "            # Reset environment to start state for each episode\n",
    "            self.env.reset()\n",
    "\n",
    "            # Loop so agent moves through state space until it reaches a terminal state (hole / end goal)\n",
    "            # Store episode path (states, actions and rewards) in episode list\n",
    "            while self.env.is_terminal(self.env.current_state()) == False:\n",
    "                state = self.env.current_state()\n",
    "                # print(state)\n",
    "                action = epsilon_soft(self.Qtable_monte, self.env, epsilon, state)\n",
    "                # Move agent based on selected action\n",
    "                \n",
    "                self.env.move(action)\n",
    "                # Retrieve reward\n",
    "                reward = self.env.get_rewards()\n",
    "\n",
    "                # Append all results to episode list so that backpropogation of rewards can be done later\n",
    "                episode.append((state, action, reward))\n",
    "\n",
    "            # Reverse episode list so looping is easier\n",
    "            episode_reversed = episode[::-1]\n",
    "            # Create a temporary list to use for checking if there are repeated visits to states\n",
    "            temp_lst = [item[0] for item in episode_reversed]\n",
    "\n",
    "            # Loop through episodes in reverse (T-1 -> T-2 -> ... -> 0)\n",
    "            for i in range(len(episode_reversed)):\n",
    "                state = episode_reversed[i][0]\n",
    "                state_f = str(state[0]) + str(state[1]) # state but formatted for referencing in dataframe\n",
    "                act_taken = episode_reversed[i][1]\n",
    "                r = episode_reversed[i][2]\n",
    "\n",
    "                G = gamma*G + r\n",
    "\n",
    "                # print(episode_reversed)\n",
    "\n",
    "                # For first visit to state, append G to the Returns dataframe\n",
    "                if state not in temp_lst[i+1:]:\n",
    "                    self.Returns.at[act_taken, state_f] = self.Returns.at[act_taken, state_f] + [G]\n",
    "                    # print(Returns.at[act_taken, state_f])\n",
    "                else:\n",
    "                    # Else, move on to next step in epsisode\n",
    "                    continue\n",
    "            \n",
    "            # Update Q table with average returns for each state and action\n",
    "            for state in self.Qtable_monte.columns.values:\n",
    "                for action in self.Qtable_monte.index.values:\n",
    "                    if len(self.Returns.at[action, state]) != 0:\n",
    "                        self.Qtable_monte.loc[action, state] = st.mean(self.Returns.at[action, state])\n",
    "                    else:    \n",
    "                        continue\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Running Simulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_monte = 0.1 # Epsilon greedy probability \n",
    "gamma_monte = 0.9 # Rewards discount rate gamma\n",
    "no_of_episodes_monte = 1000 # Number of episodes to be executed in simulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Creating instance of class & running simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m monte \u001b[39m=\u001b[39m monte_carlo_sim(env1)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Run Monte Carlo simulation\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m monte\u001b[39m.\u001b[39;49msimulate(no_of_episodes_monte, epsilon_monte, gamma_monte)\n",
      "Cell \u001b[1;32mIn[130], line 82\u001b[0m, in \u001b[0;36mmonte_carlo_sim.simulate\u001b[1;34m(self, no_of_episodes, epsilon, gamma)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mfor\u001b[39;00m action \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQtable_monte\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mvalues:\n\u001b[0;32m     81\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mReturns\u001b[39m.\u001b[39mat[action, state]) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 82\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQtable_monte\u001b[39m.\u001b[39mloc[action, state] \u001b[39m=\u001b[39m st\u001b[39m.\u001b[39mmean(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mReturns\u001b[39m.\u001b[39mat[action, state])\n\u001b[0;32m     83\u001b[0m     \u001b[39melse\u001b[39;00m:    \n\u001b[0;32m     84\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moses\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    669\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 670\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value)\n",
      "File \u001b[1;32mc:\\Users\\moses\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1765\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   1761\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1762\u001b[0m \n\u001b[0;32m   1763\u001b[0m         \u001b[39m# scalar value\u001b[39;00m\n\u001b[0;32m   1764\u001b[0m         \u001b[39mfor\u001b[39;00m loc \u001b[39min\u001b[39;00m ilocs:\n\u001b[1;32m-> 1765\u001b[0m             isetter(loc, value)\n\u001b[0;32m   1767\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1768\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(indexer, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m   1769\u001b[0m \n\u001b[0;32m   1770\u001b[0m         \u001b[39m# if we are setting on the info axis ONLY\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m         \u001b[39m# set using those methods to avoid block-splitting\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m         \u001b[39m# logic here\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moses\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1695\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer.<locals>.isetter\u001b[1;34m(loc, v)\u001b[0m\n\u001b[0;32m   1692\u001b[0m     ser\u001b[39m.\u001b[39m_maybe_update_cacher(clear\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1694\u001b[0m \u001b[39m# reset the sliced object if unique\u001b[39;00m\n\u001b[1;32m-> 1695\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_iset_item(loc, ser)\n",
      "File \u001b[1;32mc:\\Users\\moses\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3097\u001b[0m, in \u001b[0;36mDataFrame._iset_item\u001b[1;34m(self, loc, value)\u001b[0m\n\u001b[0;32m   3094\u001b[0m \u001b[39m# technically _sanitize_column expects a label, not a position,\u001b[39;00m\n\u001b[0;32m   3095\u001b[0m \u001b[39m#  but the behavior is the same as long as we pass broadcast=False\u001b[39;00m\n\u001b[0;32m   3096\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sanitize_column(loc, value, broadcast\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m-> 3097\u001b[0m NDFrame\u001b[39m.\u001b[39;49m_iset_item(\u001b[39mself\u001b[39;49m, loc, value)\n\u001b[0;32m   3099\u001b[0m \u001b[39m# check if we are modifying a copy\u001b[39;00m\n\u001b[0;32m   3100\u001b[0m \u001b[39m# try to set first as we want an invalid\u001b[39;00m\n\u001b[0;32m   3101\u001b[0m \u001b[39m# value exception to occur first\u001b[39;00m\n\u001b[0;32m   3102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\moses\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3569\u001b[0m, in \u001b[0;36mNDFrame._iset_item\u001b[1;34m(self, loc, value)\u001b[0m\n\u001b[0;32m   3568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_iset_item\u001b[39m(\u001b[39mself\u001b[39m, loc: \u001b[39mint\u001b[39m, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 3569\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49miset(loc, value)\n\u001b[0;32m   3570\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\Users\\moses\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1086\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[1;34m(self, loc, value)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     loc \u001b[39m=\u001b[39m [loc]\n\u001b[0;32m   1085\u001b[0m \u001b[39m# Accessing public blknos ensures the public versions are initialized\u001b[39;00m\n\u001b[1;32m-> 1086\u001b[0m blknos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblknos[loc]\n\u001b[0;32m   1087\u001b[0m blklocs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblklocs[loc]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m   1089\u001b[0m unfit_mgr_locs \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create instance of class with environment 1 (4x4 grid)\n",
    "monte = monte_carlo_sim(env1)\n",
    "\n",
    "# Run Monte Carlo simulation\n",
    "monte.simulate(no_of_episodes_monte, epsilon_monte, gamma_monte)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Qtable Results for *Monte Carlo Simulation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.417378</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.428551</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.514326</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.747672</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.008802</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.084387</td>\n",
       "      <td>0</td>\n",
       "      <td>0.427347</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.047179</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.729000</td>\n",
       "      <td>0.697313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039713</td>\n",
       "      <td>-0.072022</td>\n",
       "      <td>-0.28243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525239</td>\n",
       "      <td>0.682830</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>-0.008031</td>\n",
       "      <td>-0.085769</td>\n",
       "      <td>-0.900000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.618794</td>\n",
       "      <td>0.742308</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.884802</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         00        01        02       03        10  11        12  13  \\\n",
       "D  0.417378 -1.000000 -0.428551 -1.00000  0.514326   0 -0.900000   0   \n",
       "U  0.000000  0.000000  0.000000  0.00000  0.008802   0 -0.084387   0   \n",
       "L  0.000000  0.039713 -0.072022 -0.28243  0.000000   0 -1.000000   0   \n",
       "R -0.008031 -0.085769 -0.900000  0.00000 -1.000000   0 -1.000000   0   \n",
       "\n",
       "         20        21        22  23  30        31        32  33  \n",
       "D -1.000000  0.747672  0.892473   0   0  0.000000  0.000000   0  \n",
       "U  0.427347 -1.000000  0.047179   0   0  0.729000  0.697313   0  \n",
       "L  0.000000  0.525239  0.682830   0   0 -1.000000  0.798600   0  \n",
       "R  0.618794  0.742308 -1.000000   0   0  0.884802  1.000000   0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.fetchQtable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA with ϵ-Greedy Behavior Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Algorithm Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA_sim:\n",
    "    '''\n",
    "    ** Please remember to reset Q and Returns table after simulation\n",
    "\n",
    "        Functions:\n",
    "        # Fetches Qtable ->                            .fetchQtable()    \n",
    "        # Run simulation ->                            .simulate(no_of_episodes, epsilon, gamma)\n",
    "        # Resets Qtable  ->                            .resettable()      \n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # Create Q table upon creating class\n",
    "        self.Qtable_sarsa = create_qtable(self.env.rows, self.env.cols, action_space)\n",
    "        \n",
    "    # Returns Qtable    \n",
    "    def fetchQtable(self):\n",
    "        return self.Qtable_sarsa\n",
    "    \n",
    "    # Resets Qtable by creating an empty table\n",
    "    def resettable(self):\n",
    "        self.Qtable_sarsa = createQtable()\n",
    "    \n",
    "    # Run SARSA simulation\n",
    "            # There will be a sub step simulation within the overarching simulation - for looking ahead and updating Qtable\n",
    "    def simulate(self, no_of_episodes, epsilon, gamma, alpha):        \n",
    "        for i in range(no_of_episodes):    \n",
    "            # reset environment to start state\n",
    "            self.env.reset()  \n",
    "\n",
    "            # Initiialise main simulation state\n",
    "            main_step_state = self.env.current_state()\n",
    "\n",
    "            # Loop for main simulation\n",
    "            while self.env.is_terminal(self.env.current_state()) == False:\n",
    "                # Assign state so that it can be referenced again later after sub step simulation is conducted\n",
    "                main_step_state = self.env.current_state()\n",
    "\n",
    "                # Loop for sub simulation - Looking ahead until agent reaches terminal state \n",
    "                while self.env.is_terminal(self.env.current_state()) == False:\n",
    "                    sub_step_state = self.env.current_state()\n",
    "                    state_formatted = str(sub_step_state[0]) + str(sub_step_state[1])\n",
    "                    # Choose action in sub step simulation\n",
    "                    sub_step_action = epsilon_soft(self.Qtable_sarsa, self.env, epsilon, sub_step_state)\n",
    "                    \n",
    "                    # Taking action - to observe next state, action and rewar\n",
    "                    self.env.move(sub_step_action)\n",
    "\n",
    "                    # Retrieve reward for taking specific action\n",
    "                    reward = self.env.get_rewards()\n",
    "                    # Retrieve new state\n",
    "                    new_sub_step_state = self.env.current_state()\n",
    "                    new_state_formatted = str(new_sub_step_state[0]) + str(new_sub_step_state[1])\n",
    "\n",
    "                    # print(\"Sub State: {}\\nSub Action: {}\\n\\n\".format(sub_step_state, sub_step_action)) ## Test print\n",
    "\n",
    "                    # Check if new state is a terminal state: If not, then retrieve action for new state\n",
    "                    if new_sub_step_state in self.env.actions:\n",
    "                        new_sub_step_action = epsilon_soft(self.Qtable_sarsa, self.env, epsilon, new_sub_step_state)\n",
    "                        \n",
    "                        # Update Q(s1, a1) in direction of Q(s2, a2)\n",
    "                        self.Qtable_sarsa.loc[sub_step_action, state_formatted] += (alpha * (reward + (gamma * self.Qtable_sarsa.at[new_sub_step_action, new_state_formatted]) - self.Qtable_sarsa.at[sub_step_action, state_formatted]))\n",
    "\n",
    "                    # If state is a terminal state, then update Q(s1, a1) with same equation, but Q(s2, a2) = 0 (since terminal state)\n",
    "                    else:\n",
    "                        self.Qtable_sarsa.loc[sub_step_action, state_formatted] += (alpha * (reward + (gamma * 0) - self.Qtable_sarsa.at[sub_step_action, state_formatted]))\n",
    "                        break # break out of sub simulation since it has reached terminal state\n",
    "\n",
    "                # Since we now want to revert back to the main simulation, we will need to reassign agent back to the main state  \n",
    "                self.env.set_state(main_step_state)\n",
    "                # Choose action based on policy with new Q values updated by sub simulation\n",
    "                main_step_action = epsilon_soft(self.Qtable_sarsa, self.env, epsilon, main_step_state)\n",
    "                # Move agent\n",
    "                self.env.move(main_step_action) \n",
    "\n",
    "                # print(\"Main state: {}\\nMain Action: {}\".format(main_step_state, main_step_action)) ## Test print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Running Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_sarsa = 0.1 # Epsilon greedy probability \n",
    "gamma_sarsa = 0.9 # Rewards discount rate gamma\n",
    "alpha_sarsa = 0.3 # Learning rate of agent\n",
    "no_of_episodes_sarsa = 100 # Number of episodes to be executed in simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Creating instance of class & running simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of class with environment 1 (4x4 grid)\n",
    "sarsa = SARSA_sim(env1)\n",
    "\n",
    "# Run SARSA simulation\n",
    "sarsa.simulate(no_of_episodes_sarsa, epsilon_sarsa, gamma_sarsa, alpha_sarsa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Qtable Results for *SARSA with ϵ-Greedy Behavior Policy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>-0.110338</td>\n",
       "      <td>-0.998372</td>\n",
       "      <td>0.585486</td>\n",
       "      <td>-0.657000</td>\n",
       "      <td>-0.139544</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770509</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.657000</td>\n",
       "      <td>0.157872</td>\n",
       "      <td>0.882982</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175025</td>\n",
       "      <td>0</td>\n",
       "      <td>0.177945</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.194971</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.631476</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.436444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038367</td>\n",
       "      <td>-0.133467</td>\n",
       "      <td>0.343927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.959646</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.176743</td>\n",
       "      <td>0.605388</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.759900</td>\n",
       "      <td>0.752662</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.418136</td>\n",
       "      <td>0.377121</td>\n",
       "      <td>0.083903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.942352</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.993218</td>\n",
       "      <td>0</td>\n",
       "      <td>0.369660</td>\n",
       "      <td>0.769369</td>\n",
       "      <td>-0.996677</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.881968</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         00        01        02        03        10  11        12  13  \\\n",
       "D -0.110338 -0.998372  0.585486 -0.657000 -0.139544   0  0.770509   0   \n",
       "U  0.000000  0.000000  0.000000  0.000000  0.175025   0  0.177945   0   \n",
       "L  0.000000  0.038367 -0.133467  0.343927  0.000000   0 -0.959646   0   \n",
       "R  0.418136  0.377121  0.083903  0.000000 -0.942352   0 -0.993218   0   \n",
       "\n",
       "         20        21        22  23  30        31        32  33  \n",
       "D -0.657000  0.157872  0.882982   0   0  0.000000  0.000000   0  \n",
       "U -0.194971 -0.300000  0.631476   0   0  0.000000  0.436444   0  \n",
       "L  0.000000 -0.176743  0.605388   0   0 -0.759900  0.752662   0  \n",
       "R  0.369660  0.769369 -0.996677   0   0  0.881968  1.000000   0  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa.fetchQtable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qlearning with an ϵ-greedy behavior policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Algorithm Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qlearning_sim:\n",
    "    '''\n",
    "    ** Please remember to reset Q and Returns table after simulation\n",
    "\n",
    "        Functions:\n",
    "        # Fetches Qtable ->                            .fetchQtable()    \n",
    "        # Run simulation ->                            .simulate(no_of_episodes, epsilon, gamma)\n",
    "        # Resets Qtable  ->                            .resettable()      \n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # Create Q table upon creating class\n",
    "        self.Qtable_qlearning = create_qtable(self.env.rows, self.env.cols, action_space)\n",
    "        \n",
    "    # Returns Qtable    \n",
    "    def fetchQtable(self):\n",
    "        return self.Qtable_qlearning\n",
    "    \n",
    "    # Resets Qtable by creating an empty table\n",
    "    def resettable(self):\n",
    "        self.Qtable_qlearning = createQtable()\n",
    "    \n",
    "    # Run Qlearning simulation\n",
    "            # There will be a sub step simulation within the overarching simulation - for looking ahead and updating Qtable\n",
    "    def simulate(self, no_of_episodes, epsilon, gamma, alpha):        \n",
    "        for i in range(no_of_episodes):    \n",
    "            # reset environment to start state\n",
    "            self.env.reset()  \n",
    "\n",
    "            # Initiialise main simulation state\n",
    "            main_step_state = self.env.current_state()\n",
    "\n",
    "            # Loop for main simulation\n",
    "            while self.env.is_terminal(self.env.current_state()) == False:\n",
    "                # Assign state so that it can be referenced again later after sub step simulation is conducted\n",
    "                main_step_state = self.env.current_state()\n",
    "\n",
    "                # Loop for sub simulation - Looking ahead until agent reaches terminal state \n",
    "                while self.env.is_terminal(self.env.current_state()) == False:\n",
    "                    sub_step_state = self.env.current_state()\n",
    "                    state_formatted = str(sub_step_state[0]) + str(sub_step_state[1])\n",
    "                    # Choose action in sub step simulation\n",
    "                    sub_step_action = epsilon_soft(self.Qtable_qlearning, self.env, epsilon, sub_step_state)\n",
    "                    \n",
    "                    # Taking action - to observe next state, action and rewar\n",
    "                    self.env.move(sub_step_action)\n",
    "\n",
    "                    # Retrieve reward for taking specific action\n",
    "                    reward = self.env.get_rewards()\n",
    "                    # Retrieve new state\n",
    "                    new_sub_step_state = self.env.current_state()\n",
    "                    new_state_formatted = str(new_sub_step_state[0]) + str(new_sub_step_state[1])\n",
    "\n",
    "                    # print(\"Sub State: {}\\nSub Action: {}\\n\\n\".format(sub_step_state, sub_step_action)) ## Test print\n",
    "\n",
    "                    # Check if new state is a terminal state: If not, then retrieve action for new state\n",
    "                    if new_sub_step_state in self.env.actions:\n",
    "                        new_sub_step_action = epsilon_soft(self.Qtable_qlearning, self.env, epsilon, new_sub_step_state)\n",
    "                        \n",
    "                        # Update Q(s1, a1) in direction of Q(s2, a2) where Q(s2, a2) is max qvalue at state s2\n",
    "                        self.Qtable_qlearning.loc[sub_step_action, state_formatted] += (alpha * (reward + (gamma * self.Qtable_qlearning[new_state_formatted].max()) - self.Qtable_qlearning.at[sub_step_action, state_formatted]))\n",
    "\n",
    "                    # If state is a terminal state, then update Q(s1, a1) with same equation, but Q(s2, a2) = 0 (since terminal state)\n",
    "                    else:\n",
    "                        self.Qtable_qlearning.loc[sub_step_action, state_formatted] += (alpha * (reward + (gamma * 0) - self.Qtable_qlearning.at[sub_step_action, state_formatted]))\n",
    "                        break # break out of sub simulation since it has reached terminal state\n",
    "\n",
    "                # Since we now want to revert back to the main simulation, we will need to reassign agent back to the main state  \n",
    "                self.env.set_state(main_step_state)\n",
    "                # Choose action based on policy with new Q values updated by sub simulation\n",
    "                main_step_action = epsilon_soft(self.Qtable_qlearning, self.env, epsilon, main_step_state)\n",
    "                # Move agent\n",
    "                self.env.move(main_step_action) \n",
    "\n",
    "                # print(\"Main state: {}\\nMain Action: {}\".format(main_step_state, main_step_action)) ## Test print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Running Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_qlearning = 0.1 # Epsilon greedy probability \n",
    "gamma_qlearning = 0.9 # Rewards discount rate gamma\n",
    "alpha_qlearning = 0.3 # Learning rate of agent\n",
    "no_of_episodes_qlearning = 100 # Number of episodes to be executed in simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Creating instance of class & running simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of class with environment 1 (4x4 grid)\n",
    "qlearning = qlearning_sim(env1)\n",
    "\n",
    "# Run Qlearning simulation\n",
    "qlearning.simulate(no_of_episodes_qlearning, epsilon_qlearning, gamma_qlearning, alpha_qlearning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Qtable Results for *Qlearning with ϵ-Greedy Behavior Policy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.207637</td>\n",
       "      <td>-0.657000</td>\n",
       "      <td>0.729000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.500739</td>\n",
       "      <td>0</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.637526</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.510000</td>\n",
       "      <td>0.714399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.215674</td>\n",
       "      <td>0.798006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>0.578814</td>\n",
       "      <td>0.652975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.990311</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552543</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.799429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.590490</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.564237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.510000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.959646</td>\n",
       "      <td>0</td>\n",
       "      <td>0.702471</td>\n",
       "      <td>0.809845</td>\n",
       "      <td>-0.971752</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.899282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         00        01        02        03        10  11        12  13  \\\n",
       "D  0.207637 -0.657000  0.729000 -0.300000  0.500739   0  0.810000   0   \n",
       "U  0.000000  0.000000  0.000000  0.000000  0.000000   0  0.637526   0   \n",
       "L  0.000000  0.510793  0.578814  0.652975  0.000000   0 -0.990311   0   \n",
       "R  0.590490  0.656100  0.564237  0.000000 -0.510000   0 -0.959646   0   \n",
       "\n",
       "         20        21        22  23  30        31        32  33  \n",
       "D -0.300000  0.000000  0.900000   0   0  0.000000  0.000000   0  \n",
       "U  0.000000 -0.510000  0.714399   0   0  0.215674  0.798006   0  \n",
       "L  0.000000  0.000000  0.552543   0   0 -0.300000  0.799429   0  \n",
       "R  0.702471  0.809845 -0.971752   0   0  0.899282  1.000000   0  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlearning.fetchQtable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "00073238948e3e314995d1c989df02e5591cbd633de70172eca577de89016890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
