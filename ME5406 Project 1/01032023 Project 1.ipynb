{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with:\n",
    "- Monte Carlo Simulation Algorithm\n",
    "- SARSA Algorithm\n",
    "- Q Learning Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment and Mission "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The goal is for the agent to learn how to navigate the state space to reach the end goal of retrieving the frisbee*\n",
    "<br></br>\n",
    "\n",
    "<U>**Within Action Space, the following actions are defined:**</U>\n",
    "\n",
    "**'L':** Move left\n",
    "\n",
    "**'D':** Move down\n",
    "\n",
    "**'R':** Move right\n",
    "\n",
    "**'U':** Move up\n",
    "\n",
    "*If agent attempts to leave the grid, when at the edges, program would set the new state as the old state. Basically it will not move\n",
    "<br></br>\n",
    "\n",
    "<U>**Map**:</U>\n",
    "    \n",
    "    S  .  .  .\n",
    "    \n",
    "    .  H  .  H\n",
    "    \n",
    "    .  .  .  H\n",
    "    \n",
    "    H  .  .  E\n",
    "<br></br>\n",
    "<U>**Rewards**:</U>\n",
    "\n",
    "Reach goal: +1\n",
    "\n",
    "Reach hole: -1\n",
    "\n",
    "Traversing frozen surface: 0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Environment\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import statistics as st\n",
    "import pandas as pd\n",
    "from queue import PriorityQueue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Grid Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Grid Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "\n",
    "    # Takes in variables of rows, cols and start state of agent\n",
    "    def __init__(self, rows, cols, start):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "    \n",
    "    # Fucntion that allows user to set rewards and actions allowed at given states\n",
    "    def set(self, rewards, actions):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "    # Function that allows user to set state of agent\n",
    "    def set_state(self,s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "    \n",
    "    # Function that fetches current state of agent\n",
    "    def current_state(self):\n",
    "        return(self.i, self.j)\n",
    "    \n",
    "    # Function that checks if agent is in a terminal state (if current state of agent is in a terminal state: hole / goal state, then function returns True)\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    # Function that fetches the possible actions the agent can take at a given state s\n",
    "    def possible_actions(self, s):\n",
    "        return self.actions[s]\n",
    "    \n",
    "    # Moves the agent in the state space based on the action taken by the agent\n",
    "    def move(self, action):\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "    \n",
    "    # Gets reward of the current state of agent\n",
    "    def get_rewards(self):\n",
    "        reward = self.rewards.get((self.i, self.j), 0)\n",
    "        return reward\n",
    "    \n",
    "    # Undo move of agent (Function isn't used but put in place if needed)\n",
    "    def undo_move(self, action):\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # assert(self.current_state() in self.all_states())\n",
    "    \n",
    "            \n",
    "    # To reset agent to be at starting state - (0, 0) in our specific example\n",
    "    def reset(self):\n",
    "        self.set_state((0,0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Characteristics class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class env_characteristics():\n",
    "    def __init__(self, no_of_rows, no_of_cols, percentage_of_holes):\n",
    "        self.rows = no_of_rows\n",
    "        self.cols = no_of_cols\n",
    "        self.actions = {}\n",
    "        self.holes = []\n",
    "        self.rewards = {}\n",
    "        self.start_state = (0, 0)\n",
    "        self.end_state = (no_of_rows - 1, no_of_cols - 1)\n",
    "        self.percentage_holes = percentage_of_holes\n",
    "     \n",
    "\n",
    "    # Function that creates legal actions dictionary (similar to hard-coded version in env1)\n",
    "    def create_legal_actions(self): \n",
    "        \n",
    "        actions = {}\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.cols):\n",
    "                action_space = ['D', 'L', 'R', 'U']\n",
    "\n",
    "                if i == 0 and j == 0:\n",
    "                    illegal_action = ('L', 'U')\n",
    "                    for item in illegal_action:\n",
    "                        action_space.remove(item)\n",
    "                    \n",
    "                    actions[(i, j)] = tuple(action_space)\n",
    "\n",
    "            \n",
    "                elif i == 0 and j == self.cols-1:\n",
    "                    illegal_action = ('R', 'U')\n",
    "                    for item in illegal_action:\n",
    "                        action_space.remove(item)\n",
    "                    actions[(i, j)] = tuple(action_space)\n",
    "                    \n",
    "                elif i == self.rows-1 and j == 0:\n",
    "                    illegal_action = ('D', 'L')\n",
    "        \n",
    "                    for item in illegal_action:\n",
    "                        action_space.remove(item)\n",
    "                    actions[(i, j)] = tuple(action_space) \n",
    "                        \n",
    "                    \n",
    "                elif i == self.rows-1 and j == self.cols-1:\n",
    "                    illegal_action = ('D', 'R')\n",
    "                    \n",
    "                    for item in illegal_action:\n",
    "                        action_space.remove(item)\n",
    "                    actions[(i, j)] = tuple(action_space) \n",
    "                                            \n",
    "\n",
    "                elif i == 0:\n",
    "                    illegal_action = ('U')\n",
    "\n",
    "                    for item in illegal_action:\n",
    "                        action_space.remove(item)\n",
    "                    actions[(i, j)] = tuple(action_space) \n",
    "                        \n",
    "\n",
    "                elif j == 0:\n",
    "                    illegal_action = ('L')\n",
    "\n",
    "                    for item in illegal_action:\n",
    "                        action_space.remove(item)\n",
    "                    actions[(i, j)] = tuple(action_space) \n",
    "\n",
    "\n",
    "                elif i == self.rows-1:\n",
    "                    illegal_action = ('D')\n",
    "\n",
    "                    for item in illegal_action:\n",
    "                        action_space.remove(item)\n",
    "                    actions[(i, j)] = tuple(action_space) \n",
    "                        \n",
    "\n",
    "                elif j == self.cols-1:\n",
    "                    illegal_action = ('R')\n",
    "\n",
    "                    for item in illegal_action:\n",
    "                        action_space.remove(item)\n",
    "                    actions[(i, j)] = tuple(action_space) \n",
    "                        \n",
    "            \n",
    "                else:\n",
    "                    illegal_action = ()\n",
    "                    \n",
    "                    for item in illegal_action:\n",
    "                        action_space.remove(item)\n",
    "                    actions[(i, j)] = tuple(action_space) \n",
    "\n",
    "\n",
    "        return actions\n",
    "    \n",
    "\n",
    "    # Funtion that creates rewards dictionary (similar to hard-coded version in env1)\n",
    "    # Rewards list will dictate where holes are placed - hence we need to ensure holes are populated without blocking\n",
    "    # We do so by running the Astar algorithm to check if there is possible path everytime we place a new hole\n",
    "    def createActionsRewards(self):\n",
    "        actions_list = self.create_legal_actions()\n",
    "        number_of_holes = int(self.percentage_holes * self.rows * self.cols)\n",
    "\n",
    "        # Create rewards dictionary\n",
    "        rewards = {}\n",
    "        hole_list = []\n",
    "\n",
    "        # Add rewards for end state\n",
    "        rewards[self.end_state] = 1\n",
    "\n",
    "        # Run loop to create correct number of holes \n",
    "        for i in range(number_of_holes):    \n",
    "            # Initialise random hole variable\n",
    "            random_hole = 0\n",
    "            \n",
    "            # Generate random hole again if random hole has already been made\n",
    "            while random_hole not in hole_list:\n",
    "                # If random hole removed from actions list already, generate another hole\n",
    "                # Generate new hole if random hole selected is start state\n",
    "                while random_hole not in actions_list or random_hole == self.start_state or random_hole == self.end_state:\n",
    "                    random_hole = (random.randint(0, self.rows - 1), random.randint(0, self.cols - 1))\n",
    "                \n",
    "                state, actions = random_hole, actions_list[random_hole]\n",
    "\n",
    "                # Delete hole from actions-list to run ASTAR algo\n",
    "                del actions_list[random_hole]\n",
    "\n",
    "                # If astar returns viable path, then append hole and note it in rewards dictionary\n",
    "                viable_path = astar_algo(self.start_state, self.end_state, self.rows, actions_list)\n",
    "                if viable_path != None:\n",
    "                    hole_list.append(state)\n",
    "                    rewards[state] = -1\n",
    "\n",
    "                # else, run main loop again\n",
    "                else:\n",
    "                    actions_list[state] = actions\n",
    "                    random_hole = 0 # Initialised hole that will never be in actions list so second while loop will run\n",
    "                \n",
    "        # Remove end state to fit definition of environment        \n",
    "        del actions_list[self.end_state] # Done here and not before because Astar algo requires endstate to be in actions_list\n",
    "        \n",
    "        return actions_list, rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Astar Algorithm for building random holes in environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def astar_algo(start_coord, end_coord, grid_size, actions):\n",
    "    \n",
    "    # Heuristic function that calculates the Manhattan distance between two points\n",
    "    def heuristic(s, e):\n",
    "        return abs(e[0] - s[0]) + abs(e[1] - s[1])\n",
    "    \n",
    "    # Converts coordinate states to number states -> (0,0) = 0, (1,0) = 4\n",
    "    def coord_to_state(coord, grid_size):\n",
    "        return coord[0] * grid_size + coord[1]\n",
    "\n",
    "    start_state = coord_to_state(start_coord, grid_size)\n",
    "    end_state = coord_to_state(end_coord, grid_size)  \n",
    "    \n",
    "    # Create g_score dict \n",
    "    g_score = {start_state: 0}\n",
    "    # Activate f_score which is g_score + heuristic score\n",
    "    f_score = {start_state: heuristic(start_coord, end_coord)}\n",
    "\n",
    "    # Create queue to select states with lowest fscore for finding optimal path\n",
    "    open_list = PriorityQueue()\n",
    "    open_list.put((f_score[start_state], start_state))\n",
    "    closed_list = set()\n",
    "\n",
    "    # came_from dictionary to return shortest path later\n",
    "    came_from = {}\n",
    "    \n",
    "    while open_list.empty() == False:\n",
    "        \n",
    "        # Get state of the lowest fscore\n",
    "        current_state = open_list.get()[1]\n",
    "\n",
    "        # Check if reached end goal\n",
    "        if current_state == end_state:\n",
    "            path = []\n",
    "\n",
    "            # If reached, return shortest path\n",
    "            while current_state != start_state:\n",
    "                path.append(current_state)\n",
    "                current_state = came_from[current_state]\n",
    "            path.append(start_state)\n",
    "            return path[::-1]\n",
    "        \n",
    "        # Add current_state to closed_list - to keep track of visited\n",
    "        closed_list.add(current_state)\n",
    "\n",
    "\n",
    "        neighbours = []\n",
    "        row, col = divmod(current_state, grid_size)\n",
    "        # print(\"current state: {}, coord: {}\".format(current_state, (row, col)))\n",
    "        \n",
    "        # Append neighbouring states to neighbours list\n",
    "        if row > 0:\n",
    "            if (row, col) in actions:\n",
    "                above_state = (row - 1, col)\n",
    "                if above_state in actions:\n",
    "                    neighbours.append(current_state - grid_size) # State above\n",
    "\n",
    "        if row < grid_size - 1:\n",
    "            if (row, col) in actions:\n",
    "                below_state = (row + 1, col)\n",
    "                if below_state in actions:\n",
    "                    neighbours.append(current_state + grid_size) # State below\n",
    "\n",
    "        if col > 0:\n",
    "            if (row, col) in actions:\n",
    "                left_state = (row, col - 1)\n",
    "                if left_state in actions:\n",
    "                    neighbours.append(current_state - 1) # State on left\n",
    "\n",
    "        if col < grid_size - 1:\n",
    "            if (row, col) in actions:\n",
    "                right_state = (row, col + 1)\n",
    "                if right_state in actions:\n",
    "                    neighbours.append(current_state + 1) # State on right\n",
    "\n",
    "        # Loop through neighbour states\n",
    "        for neighbour_state in neighbours:\n",
    "            if neighbour_state in closed_list:\n",
    "                continue\n",
    "        \n",
    "            # Calculate tentative g-score of neighbour\n",
    "            tentative_g_score = g_score[current_state] + 1\n",
    "            if neighbour_state not in g_score or tentative_g_score < g_score[neighbour_state]:\n",
    "                # Update scores and came_from dictionary\n",
    "                g_score[neighbour_state] = tentative_g_score\n",
    "                f_score[neighbour_state] = tentative_g_score + heuristic((neighbour_state // grid_size, neighbour_state % grid_size), end_coord)\n",
    "                came_from[neighbour_state] = current_state \n",
    "\n",
    "                open_list.put((f_score[neighbour_state], neighbour_state))\n",
    "\n",
    "    # If no path, return none\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Grid Environment Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_grid(rewards, actions, rows, cols, start_state):\n",
    "    # define a grid that describes the reward for arriving at each state\n",
    "    # and possible actions at each state\n",
    "    # the grid looks like this\n",
    "    # S means start position\n",
    "    # E means the end states\n",
    "\n",
    "        # S  .  .  .\n",
    "        # .  H  .  H\n",
    "        # .  .  .  H\n",
    "        # H  .  .  E\n",
    "\n",
    "    g = Grid(rows, cols, start_state) #(rows, cols, start_state)\n",
    "    g.set(rewards, actions)\n",
    "    return g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Environment 1 - 4 x 4 Grid (With specific holes in environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Characteristics\n",
    "# no. of rows & cols of grid\n",
    "no_of_rows = 4\n",
    "no_of_cols = 4\n",
    "\n",
    "# Full action space\n",
    "action_space = ('D', 'U', 'L', 'R')\n",
    "\n",
    "# Assigned start state\n",
    "start_state = (0, 0)\n",
    "\n",
    "# Define rewards at specific states (punishment yields negative rewards)\n",
    "# rewards at given states (in dictionary form)\n",
    "rewards = {(1, 1): -1, # hole\n",
    "           (1, 3): -1, # hole\n",
    "           (2, 3): -1, # hole\n",
    "           (3, 0): -1, # hole\n",
    "           (3, 3): 1} # frisbee\n",
    "\n",
    "# Define legal (possible) actions at each state\n",
    "# States that depict terminal state (hole / end goal) are commented because this will tie in with the .is_terminal() function under class Grid\n",
    "actions = {\n",
    "        (0, 0): ('D', 'R'), # Start_state\n",
    "        (0, 1): ('D', 'R', 'L'), \n",
    "        (0, 2): ('D', 'R', 'L'),\n",
    "        (0, 3): ('D', 'L'),\n",
    "        (1, 0): ('D', 'R', 'U'),\n",
    "        #(1, 1): ('D', 'R', 'L', 'U'), #Hole\n",
    "        (1, 2): ('D', 'R', 'L', 'U'),\n",
    "        #(1, 3): ('D', 'U', 'L'), #Hole\n",
    "        (2, 0): ('D', 'U', 'R'),\n",
    "        (2, 1): ('D', 'R', 'L', 'U'),\n",
    "        (2, 2): ('D', 'R', 'L', 'U'),\n",
    "        #(2, 3): ('D', 'U', 'L'), #Hole\n",
    "        #(3, 0): ('U', 'R', ), #Hole\n",
    "        (3, 1): ('U', 'R', 'L'),\n",
    "        (3, 2): ('U', 'R', 'L'),\n",
    "        #(3, 3): (), #End-State (frisbee)\n",
    "}\n",
    "\n",
    "\n",
    "# Create 4x4 Grid environment\n",
    "env1 = standard_grid(rewards, actions, no_of_rows, no_of_cols, start_state) \n",
    "\n",
    "# Reset environment to start state defined as (0,0) in .reset() function\n",
    "env1.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --- Function testing ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "Original State: (0, 0), After taking action: (1, 0)\n",
      "State before: (1, 0), State After taking action 'L': (1, 0)\n",
      "State before: (1, 0), State After taking action 'R': (1, 1)\n",
      "Reached terminal state (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test if .is_terminal() function works\n",
    "    # Terminal States: 1,1  1,3  2,3  3,0  3,3\n",
    "print(env1.is_terminal((2, 0)))\n",
    "print(env1.is_terminal((3, 0)))\n",
    "\n",
    "\n",
    "# Test .move()\n",
    "env1.reset()\n",
    "state_before = env1.current_state()\n",
    "action = env1.move('D')\n",
    "state_after = env1.current_state()\n",
    "print('Original State: {}, After taking action: {}'.format(state_before, state_after))\n",
    "\n",
    "\n",
    "# Test loop to stop moving when environment reaches terminal state\n",
    "while env1.is_terminal(env1.current_state()) == False:\n",
    "    a = action_space[(random.randint(0, (len(action_space)-1)))]\n",
    "    state_b = env1.current_state()\n",
    "    env1.move(a)\n",
    "    state_a = env1.current_state()\n",
    "    \n",
    "    print(\"State before: {}, State After taking action '{}': {}\".format(state_b, a, state_a))\n",
    "\n",
    "else:\n",
    "    print('Reached terminal state {}'.format(env1.current_state()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extended Implementation: Creating Environment 2 - 10 x 10 Grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Creating Env2, Legal Actions and Rewards List  </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 5\n",
    "ncols = 5\n",
    "p_holes = 0.25\n",
    "start_state_env_2 = (0,0)\n",
    "\n",
    "# Creating legal actions and rewards list to parse into standard_grid class\n",
    "actions_env2, rewards_env2 = env_characteristics(nrows, ncols, p_holes).createActionsRewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10x10 Grid environment\n",
    "env2 = standard_grid(rewards_env2, actions_env2, nrows, ncols, start_state_env_2) \n",
    "\n",
    "# Reset environment to start state defined as (0,0) in .reset() function\n",
    "env2.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --- Function testing ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "Original State: (0, 0), After taking action: (1, 0)\n",
      "State before: (1, 0), State After taking action 'D': (2, 0)\n",
      "State before: (2, 0), State After taking action 'R': (2, 1)\n",
      "State before: (2, 1), State After taking action 'D': (3, 1)\n",
      "State before: (3, 1), State After taking action 'U': (2, 1)\n",
      "State before: (2, 1), State After taking action 'L': (2, 0)\n",
      "State before: (2, 0), State After taking action 'R': (2, 1)\n",
      "State before: (2, 1), State After taking action 'L': (2, 0)\n",
      "State before: (2, 0), State After taking action 'U': (1, 0)\n",
      "State before: (1, 0), State After taking action 'D': (2, 0)\n",
      "State before: (2, 0), State After taking action 'R': (2, 1)\n",
      "State before: (2, 1), State After taking action 'L': (2, 0)\n",
      "State before: (2, 0), State After taking action 'L': (2, 0)\n",
      "State before: (2, 0), State After taking action 'U': (1, 0)\n",
      "State before: (1, 0), State After taking action 'U': (0, 0)\n",
      "State before: (0, 0), State After taking action 'R': (0, 1)\n",
      "State before: (0, 1), State After taking action 'L': (0, 0)\n",
      "State before: (0, 0), State After taking action 'U': (0, 0)\n",
      "State before: (0, 0), State After taking action 'L': (0, 0)\n",
      "State before: (0, 0), State After taking action 'L': (0, 0)\n",
      "State before: (0, 0), State After taking action 'U': (0, 0)\n",
      "State before: (0, 0), State After taking action 'R': (0, 1)\n",
      "State before: (0, 1), State After taking action 'R': (0, 2)\n",
      "State before: (0, 2), State After taking action 'R': (0, 3)\n",
      "State before: (0, 3), State After taking action 'U': (0, 3)\n",
      "State before: (0, 3), State After taking action 'U': (0, 3)\n",
      "State before: (0, 3), State After taking action 'U': (0, 3)\n",
      "State before: (0, 3), State After taking action 'R': (0, 4)\n",
      "Reached terminal state (0, 4)\n"
     ]
    }
   ],
   "source": [
    "# Test if .is_terminal() function works\n",
    "    # Terminal States: 1,1  1,3  2,3  3,0  3,3\n",
    "print(env2.is_terminal((2, 0)))\n",
    "print(env2.is_terminal((3, 0)))\n",
    "\n",
    "\n",
    "# Test .move()\n",
    "env2.reset()\n",
    "state_before = env2.current_state()\n",
    "action = env2.move('D')\n",
    "state_after = env2.current_state()\n",
    "print('Original State: {}, After taking action: {}'.format(state_before, state_after))\n",
    "\n",
    "\n",
    "# Test loop to stop moving when environment reaches terminal state\n",
    "while env2.is_terminal(env2.current_state()) == False:\n",
    "    a = action_space[(random.randint(0, (len(action_space)-1)))]\n",
    "    state_b = env2.current_state()\n",
    "    env2.move(a)\n",
    "    state_a = env2.current_state()\n",
    "    \n",
    "    print(\"State before: {}, State After taking action '{}': {}\".format(state_b, a, state_a))\n",
    "\n",
    "else:\n",
    "    print('Reached terminal state {}'.format(env2.current_state()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Characteristics class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q table, Returns table and Policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q table Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q table is built as a dataframe for easier referencing: there were problems with referencing when building a multi nested dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qtable(no_of_rows, no_of_cols, action_space):\n",
    "    # Creates Q table as a nested dictionary\n",
    "    Q = {}\n",
    "    for i in range(no_of_rows):\n",
    "        for j in range(no_of_cols):\n",
    "            Q[(str(i) + str(j))] = 0\n",
    "    \n",
    "    action_space_dic = {}\n",
    "    for item in action_space:\n",
    "        action_space_dic[item] = 0\n",
    "        \n",
    "\n",
    "    for k, v in Q.items():\n",
    "        Q[k] = action_space_dic\n",
    "    \n",
    "    # Converts Q table into a dataframe\n",
    "    Q = pd.DataFrame(data = Q)\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns table Function\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Returns table is built as a dataframe for easier referencing: there were problems with referencing when building a multi nested dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_returnstable(no_of_rows, no_of_cols, action_space):\n",
    "    # Creates Returns table as a nested dictionary\n",
    "    returns = {}\n",
    "    for i in range(no_of_rows):\n",
    "        for j in range(no_of_cols):\n",
    "            returns[(str(i) + str(j))] = 0\n",
    "    \n",
    "    action_space_dic = {}\n",
    "    for item in action_space:\n",
    "        action_space_dic[item] = []\n",
    "        \n",
    "\n",
    "    for k, v in returns.items():\n",
    "        returns[k] = action_space_dic\n",
    "    \n",
    "    # Converts Returns table into a dataframe\n",
    "    returns = pd.DataFrame(data = returns)\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an action for the agent to take. Each action has a minimum probability of (epsilon / no. of actions) of being selected\n",
    "# Optimal action has a higher probability of being selected\n",
    "def epsilon_soft(Qtable, env, epsilon, currentstate):\n",
    "\n",
    "    prob = epsilon # sum of minimum prob of selecting all actions in action space\n",
    "\n",
    "    # Set a random probability to determine which actions are being selected\n",
    "    random_prob = random.random()\n",
    "    best_actions = []\n",
    "    valid_q_values = []\n",
    "\n",
    "    # Finding max q value at the specific state for legal actions\n",
    "    state_f = str(currentstate[0]) + str(currentstate[1]) # Formatted state\n",
    "    for legal_action in env.actions[currentstate]:\n",
    "        valid_q_values.append(Qtable.at[legal_action, state_f])\n",
    "        max_q_value = max(valid_q_values)\n",
    "\n",
    "    # Appending max q value of legal actions to best_actions list\n",
    "    for item in Qtable[Qtable[state_f] == max_q_value].index.values:\n",
    "        if item in env.actions[currentstate]:\n",
    "            best_actions.append(item)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # When random_prob =< sum of min prob of all actions, randomly select action\n",
    "    if random_prob <= prob:\n",
    "        # Select random action from legal actions\n",
    "        valid_actions = env.actions[currentstate]\n",
    "        action = valid_actions[(random.randint(0, (len(valid_actions)-1)))]\n",
    "        return action\n",
    "            \n",
    "    # If random_prob > prob, then select legal action with highest q value\n",
    "        # Other problems that this code solves:\n",
    "            # 1. When more than 1 action has the same q value - select the action randomly\n",
    "    else:\n",
    "        action = best_actions[random.randint(0, len(best_actions)-1)]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-visit Monte Carlo Without Exploring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Algorithm Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class monte_carlo_sim:\n",
    "    '''\n",
    "    ** Please remember to reset Q and Returns table after simulation\n",
    "\n",
    "        Functions:\n",
    "        # Fetches Qtable ->                            .fetchQtable()    \n",
    "        # Fetches Returns Table ->                     .fetchReturnstable() \n",
    "        # Run simulation ->                            .simulate(no_of_episodes, epsilon, gamma)\n",
    "        # Resets Qtable and Returns Table ->           .resettables()      \n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # Create Q and Returns table on creating class\n",
    "        self.Qtable_monte = create_qtable(self.env.rows, self.env.cols, action_space)\n",
    "        self.Returns = create_returnstable(self.env.rows, self.env.cols, action_space)\n",
    "\n",
    "    # Returns Qtable\n",
    "    def fetchQtable(self):\n",
    "        return self.Qtable_monte\n",
    "    \n",
    "    # Returns Returns table\n",
    "    def fetchReturnstable(self):\n",
    "        return self.Returns\n",
    "    \n",
    "    # Reset Q and Returns table by creating empty tables\n",
    "    def resettables(self):\n",
    "        self.Qtable_monte = createQtable()\n",
    "        self.Returns = createReturnstable()\n",
    "\n",
    "    # Run monte carlo simulation\n",
    "    def simulate(self, no_of_episodes, epsilon, gamma):\n",
    "        for i in range(no_of_episodes):\n",
    "            episode = []\n",
    "            G = 0\n",
    "\n",
    "            # Reset environment to start state for each episode\n",
    "            self.env.reset()\n",
    "\n",
    "            # Loop so agent moves through state space until it reaches a terminal state (hole / end goal)\n",
    "            # Store episode path (states, actions and rewards) in episode list\n",
    "            while self.env.is_terminal(self.env.current_state()) == False:\n",
    "                state = self.env.current_state()\n",
    "                # print(state)\n",
    "                action = epsilon_soft(self.Qtable_monte, self.env, epsilon, state)\n",
    "                # Move agent based on selected action\n",
    "                \n",
    "                self.env.move(action)\n",
    "                # Retrieve reward\n",
    "                reward = self.env.get_rewards()\n",
    "\n",
    "                # Append all results to episode list so that backpropogation of rewards can be done later\n",
    "                episode.append((state, action, reward))\n",
    "\n",
    "            # Reverse episode list so looping is easier\n",
    "            episode_reversed = episode[::-1]\n",
    "            # Create a temporary list to use for checking if there are repeated visits to states\n",
    "            temp_lst = [item[0] for item in episode_reversed]\n",
    "\n",
    "            # Loop through episodes in reverse (T-1 -> T-2 -> ... -> 0)\n",
    "            for i in range(len(episode_reversed)):\n",
    "                state = episode_reversed[i][0]\n",
    "                state_f = str(state[0]) + str(state[1]) # state but formatted for referencing in dataframe\n",
    "                act_taken = episode_reversed[i][1]\n",
    "                r = episode_reversed[i][2]\n",
    "\n",
    "                G = gamma*G + r\n",
    "\n",
    "                # print(episode_reversed)\n",
    "\n",
    "                # For first visit to state, append G to the Returns dataframe\n",
    "                if state not in temp_lst[i+1:]:\n",
    "                    self.Returns.at[act_taken, state_f] = self.Returns.at[act_taken, state_f] + [G]\n",
    "                    # print(Returns.at[act_taken, state_f])\n",
    "                else:\n",
    "                    # Else, move on to next step in epsisode\n",
    "                    continue\n",
    "            \n",
    "            # Update Q table with average returns for each state and action\n",
    "            for state in self.Qtable_monte.columns.values:\n",
    "                for action in self.Qtable_monte.index.values:\n",
    "                    if len(self.Returns.at[action, state]) != 0:\n",
    "                        self.Qtable_monte.loc[action, state] = st.mean(self.Returns.at[action, state])\n",
    "                    else:    \n",
    "                        continue\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Running Simulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_monte = 0.1 # Epsilon greedy probability \n",
    "gamma_monte = 0.9 # Rewards discount rate gamma\n",
    "no_of_episodes_monte = 100 # Number of episodes to be executed in simulation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Creating instance of class & running simulation (Environment 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of class with environment 1 (4x4 grid)\n",
    "monte1 = monte_carlo_sim(env1)\n",
    "\n",
    "# Run Monte Carlo simulation\n",
    "monte1.simulate(no_of_episodes_monte, epsilon_monte, gamma_monte)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Qtable Results for *Monte Carlo Simulation* (Environment 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>[-0.7290000000000001, -0.81, -0.20589113209464...</td>\n",
       "      <td>[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>[-0.5904900000000002, -0.05814973700304011, -0...</td>\n",
       "      <td>[-1.0]</td>\n",
       "      <td>[-0.6561000000000001, -0.9, -0.590490000000000...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.6561000000000001, -0.28242953648100017, -0....</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>[-0.81, -0.7290000000000001, -0.9, -0.00106111...</td>\n",
       "      <td>[-0.2058911320946491]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.81, -0.34867844010000015, -0.5314410000000...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.6561000000000001]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.6561000000000001, -0.001310020508637622, -...</td>\n",
       "      <td>[-1.0]</td>\n",
       "      <td>[0.5904900000000002, -0.25418658283290013, -0....</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.9]</td>\n",
       "      <td>[0.5314410000000002, -0.001310020508637622, -0...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.03433683820292516, -0.43046721000000016, -...</td>\n",
       "      <td>[-0.31381059609000017, -0.05814973700304011, -...</td>\n",
       "      <td>[-0.81, -0.0041745579179292966, -0.00221853123...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-1.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.81, 0.34867844010000015, -0.28242953648100...</td>\n",
       "      <td>[-0.0014555783429306911, -0.0523347633027361, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-1.0]</td>\n",
       "      <td>[-0.9]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>[-0.5904900000000002, -0.030903154382632643, -...</td>\n",
       "      <td>[-0.6561000000000001, -0.5314410000000002, -6....</td>\n",
       "      <td>[-0.7290000000000001, -0.9, -0.001996678111016...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-1.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.7290000000000001, -0.81, -0.25418658283290...</td>\n",
       "      <td>[-0.9, -0.22876792454961012, -0.18530201888518...</td>\n",
       "      <td>[-1.0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.81, 0.47829690000000014, -0.00117901845777...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  00  \\\n",
       "D  [-0.7290000000000001, -0.81, -0.20589113209464...   \n",
       "U                                                 []   \n",
       "L                                                 []   \n",
       "R  [-0.5904900000000002, -0.030903154382632643, -...   \n",
       "\n",
       "                                                  01  \\\n",
       "D  [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1....   \n",
       "U                                                 []   \n",
       "L  [-0.03433683820292516, -0.43046721000000016, -...   \n",
       "R  [-0.6561000000000001, -0.5314410000000002, -6....   \n",
       "\n",
       "                                                  02  \\\n",
       "D  [-0.5904900000000002, -0.05814973700304011, -0...   \n",
       "U                                                 []   \n",
       "L  [-0.31381059609000017, -0.05814973700304011, -...   \n",
       "R  [-0.7290000000000001, -0.9, -0.001996678111016...   \n",
       "\n",
       "                                                  03  \\\n",
       "D                                             [-1.0]   \n",
       "U                                                 []   \n",
       "L  [-0.81, -0.0041745579179292966, -0.00221853123...   \n",
       "R                                                 []   \n",
       "\n",
       "                                                  10  11  \\\n",
       "D  [-0.6561000000000001, -0.9, -0.590490000000000...  []   \n",
       "U  [-0.81, -0.34867844010000015, -0.5314410000000...  []   \n",
       "L                                                 []  []   \n",
       "R         [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]  []   \n",
       "\n",
       "                                                  12  13  \\\n",
       "D  [0.6561000000000001, -0.28242953648100017, -0....  []   \n",
       "U                              [-0.6561000000000001]  []   \n",
       "L                                             [-1.0]  []   \n",
       "R                                             [-1.0]  []   \n",
       "\n",
       "                                                  20  \\\n",
       "D  [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1....   \n",
       "U  [-0.6561000000000001, -0.001310020508637622, -...   \n",
       "L                                                 []   \n",
       "R  [-0.7290000000000001, -0.81, -0.25418658283290...   \n",
       "\n",
       "                                                  21  \\\n",
       "D  [-0.81, -0.7290000000000001, -0.9, -0.00106111...   \n",
       "U                                             [-1.0]   \n",
       "L  [-0.81, 0.34867844010000015, -0.28242953648100...   \n",
       "R  [-0.9, -0.22876792454961012, -0.18530201888518...   \n",
       "\n",
       "                                                  22  23  30  \\\n",
       "D                              [-0.2058911320946491]  []  []   \n",
       "U  [0.5904900000000002, -0.25418658283290013, -0....  []  []   \n",
       "L  [-0.0014555783429306911, -0.0523347633027361, ...  []  []   \n",
       "R                                             [-1.0]  []  []   \n",
       "\n",
       "                                                  31  \\\n",
       "D                                                 []   \n",
       "U                                             [-0.9]   \n",
       "L                                             [-1.0]   \n",
       "R  [-0.81, 0.47829690000000014, -0.00117901845777...   \n",
       "\n",
       "                                                  32  33  \n",
       "D                                                 []  []  \n",
       "U  [0.5314410000000002, -0.001310020508637622, -0...  []  \n",
       "L                                             [-0.9]  []  \n",
       "R                                                 []  []  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte1.fetchQtable()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Extended Implementation: Creating instance of class & running simulation (Environment 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'51'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\moses\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:2895\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   2896\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas\\_libs\\index.pyx:70\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\index.pyx:101\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:1675\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:1683\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '51'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[221], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m monte2 \u001b[39m=\u001b[39m monte_carlo_sim(env2)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Run Monte Carlo simulation\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m monte2\u001b[39m.\u001b[39;49msimulate(no_of_episodes_monte, epsilon_monte, gamma_monte)\n",
      "Cell \u001b[1;32mIn[207], line 44\u001b[0m, in \u001b[0;36mmonte_carlo_sim.simulate\u001b[1;34m(self, no_of_episodes, epsilon, gamma)\u001b[0m\n\u001b[0;32m     42\u001b[0m state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mcurrent_state()\n\u001b[0;32m     43\u001b[0m \u001b[39m# print(state)\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m action \u001b[39m=\u001b[39m epsilon_soft(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mQtable_monte, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, epsilon, state)\n\u001b[0;32m     45\u001b[0m \u001b[39m# Move agent based on selected action\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mmove(action)\n",
      "Cell \u001b[1;32mIn[211], line 15\u001b[0m, in \u001b[0;36mepsilon_soft\u001b[1;34m(Qtable, env, epsilon, currentstate)\u001b[0m\n\u001b[0;32m     13\u001b[0m state_f \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(currentstate[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(currentstate[\u001b[39m1\u001b[39m]) \u001b[39m# Formatted state\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m legal_action \u001b[39min\u001b[39;00m env\u001b[39m.\u001b[39mactions[currentstate]:\n\u001b[1;32m---> 15\u001b[0m     valid_q_values\u001b[39m.\u001b[39mappend(Qtable\u001b[39m.\u001b[39;49mat[legal_action, state_f])\n\u001b[0;32m     16\u001b[0m     max_q_value \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(valid_q_values)\n\u001b[0;32m     18\u001b[0m \u001b[39m# Appending max q value of legal actions to best_actions list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moses\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:2080\u001b[0m, in \u001b[0;36m_AtIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2077\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid call for scalar access (getting)!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2078\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39mloc[key]\n\u001b[1;32m-> 2080\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(key)\n",
      "File \u001b[1;32mc:\\Users\\moses\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:2027\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2024\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid call for scalar access (getting)!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2026\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_key(key)\n\u001b[1;32m-> 2027\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_get_value(\u001b[39m*\u001b[39;49mkey, takeable\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_takeable)\n",
      "File \u001b[1;32mc:\\Users\\moses\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3006\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3003\u001b[0m     series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ixs(col, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m   3004\u001b[0m     \u001b[39mreturn\u001b[39;00m series\u001b[39m.\u001b[39m_values[index]\n\u001b[1;32m-> 3006\u001b[0m series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_item_cache(col)\n\u001b[0;32m   3007\u001b[0m engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_engine\n\u001b[0;32m   3009\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\moses\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3540\u001b[0m, in \u001b[0;36mNDFrame._get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   3535\u001b[0m res \u001b[39m=\u001b[39m cache\u001b[39m.\u001b[39mget(item)\n\u001b[0;32m   3536\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3537\u001b[0m     \u001b[39m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[0;32m   3538\u001b[0m     \u001b[39m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[1;32m-> 3540\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(item)\n\u001b[0;32m   3541\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39miget(loc)\n\u001b[0;32m   3542\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_box_col_values(values, loc)\n",
      "File \u001b[1;32mc:\\Users\\moses\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:2897\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   2896\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 2897\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   2899\u001b[0m \u001b[39mif\u001b[39;00m tolerance \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2900\u001b[0m     tolerance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_tolerance(tolerance, np\u001b[39m.\u001b[39masarray(key))\n",
      "\u001b[1;31mKeyError\u001b[0m: '51'"
     ]
    }
   ],
   "source": [
    "# Create instance of class with environment 1 (10x10 grid)\n",
    "monte2 = monte_carlo_sim(env2)\n",
    "\n",
    "# Run Monte Carlo simulation\n",
    "monte2.simulate(no_of_episodes_monte, epsilon_monte, gamma_monte)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Qtable Results for *Monte Carlo Simulation* (Environment 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>[-0.6561000000000001]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.7290000000000001]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      00  01  02  03  04  05                     10  11  12  \\\n",
       "D  [-0.6561000000000001]  []  []  []  []  []  [-0.7290000000000001]  []  []   \n",
       "U                     []  []  []  []  []  []                     []  []  []   \n",
       "L                     []  []  []  []  []  []                     []  []  []   \n",
       "R                     []  []  []  []  []  []                     []  []  []   \n",
       "\n",
       "   13  ...  42  43  44  45  50  51  52  53  54  55  \n",
       "D  []  ...  []  []  []  []  []  []  []  []  []  []  \n",
       "U  []  ...  []  []  []  []  []  []  []  []  []  []  \n",
       "L  []  ...  []  []  []  []  []  []  []  []  []  []  \n",
       "R  []  ...  []  []  []  []  []  []  []  []  []  []  \n",
       "\n",
       "[4 rows x 36 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte2.fetchReturnstable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA with ϵ-Greedy Behavior Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Algorithm Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA_sim:\n",
    "    '''\n",
    "    ** Please remember to reset Q and Returns table after simulation\n",
    "\n",
    "        Functions:\n",
    "        # Fetches Qtable ->                            .fetchQtable()    \n",
    "        # Run simulation ->                            .simulate(no_of_episodes, epsilon, gamma)\n",
    "        # Resets Qtable  ->                            .resettable()      \n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # Create Q table upon creating class\n",
    "        self.Qtable_sarsa = create_qtable(self.env.rows, self.env.cols, action_space)\n",
    "        \n",
    "    # Returns Qtable    \n",
    "    def fetchQtable(self):\n",
    "        return self.Qtable_sarsa\n",
    "    \n",
    "    # Resets Qtable by creating an empty table\n",
    "    def resettable(self):\n",
    "        self.Qtable_sarsa = createQtable()\n",
    "    \n",
    "    # Run SARSA simulation\n",
    "            # There will be a sub step simulation within the overarching simulation - for looking ahead and updating Qtable\n",
    "    def simulate(self, no_of_episodes, epsilon, gamma, alpha):        \n",
    "        for i in range(no_of_episodes):    \n",
    "            # reset environment to start state\n",
    "            self.env.reset()  \n",
    "\n",
    "            # Initiialise main simulation state\n",
    "            main_step_state = self.env.current_state()\n",
    "\n",
    "            # Loop for main simulation\n",
    "            while self.env.is_terminal(self.env.current_state()) == False:\n",
    "                # Assign state so that it can be referenced again later after sub step simulation is conducted\n",
    "                main_step_state = self.env.current_state()\n",
    "\n",
    "                # Loop for sub simulation - Looking ahead until agent reaches terminal state \n",
    "                while self.env.is_terminal(self.env.current_state()) == False:\n",
    "                    sub_step_state = self.env.current_state()\n",
    "                    state_formatted = str(sub_step_state[0]) + str(sub_step_state[1])\n",
    "                    # Choose action in sub step simulation\n",
    "                    sub_step_action = epsilon_soft(self.Qtable_sarsa, self.env, epsilon, sub_step_state)\n",
    "                    \n",
    "                    # Taking action - to observe next state, action and rewar\n",
    "                    self.env.move(sub_step_action)\n",
    "\n",
    "                    # Retrieve reward for taking specific action\n",
    "                    reward = self.env.get_rewards()\n",
    "                    # Retrieve new state\n",
    "                    new_sub_step_state = self.env.current_state()\n",
    "                    new_state_formatted = str(new_sub_step_state[0]) + str(new_sub_step_state[1])\n",
    "\n",
    "                    # print(\"Sub State: {}\\nSub Action: {}\\n\\n\".format(sub_step_state, sub_step_action)) ## Test print\n",
    "\n",
    "                    # Check if new state is a terminal state: If not, then retrieve action for new state\n",
    "                    if new_sub_step_state in self.env.actions:\n",
    "                        new_sub_step_action = epsilon_soft(self.Qtable_sarsa, self.env, epsilon, new_sub_step_state)\n",
    "                        \n",
    "                        # Update Q(s1, a1) in direction of Q(s2, a2)\n",
    "                        self.Qtable_sarsa.loc[sub_step_action, state_formatted] += (alpha * (reward + (gamma * self.Qtable_sarsa.at[new_sub_step_action, new_state_formatted]) - self.Qtable_sarsa.at[sub_step_action, state_formatted]))\n",
    "\n",
    "                    # If state is a terminal state, then update Q(s1, a1) with same equation, but Q(s2, a2) = 0 (since terminal state)\n",
    "                    else:\n",
    "                        self.Qtable_sarsa.loc[sub_step_action, state_formatted] += (alpha * (reward + (gamma * 0) - self.Qtable_sarsa.at[sub_step_action, state_formatted]))\n",
    "                        break # break out of sub simulation since it has reached terminal state\n",
    "\n",
    "                # Since we now want to revert back to the main simulation, we will need to reassign agent back to the main state  \n",
    "                self.env.set_state(main_step_state)\n",
    "                # Choose action based on policy with new Q values updated by sub simulation\n",
    "                main_step_action = epsilon_soft(self.Qtable_sarsa, self.env, epsilon, main_step_state)\n",
    "                # Move agent\n",
    "                self.env.move(main_step_action) \n",
    "\n",
    "                # print(\"Main state: {}\\nMain Action: {}\".format(main_step_state, main_step_action)) ## Test print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Running Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_sarsa = 0.1 # Epsilon greedy probability \n",
    "gamma_sarsa = 0.9 # Rewards discount rate gamma\n",
    "alpha_sarsa = 0.3 # Learning rate of agent\n",
    "no_of_episodes_sarsa = 100 # Number of episodes to be executed in simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Creating instance of class & running simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of class with environment 1 (4x4 grid)\n",
    "sarsa = SARSA_sim(env1)\n",
    "\n",
    "# Run SARSA simulation\n",
    "sarsa.simulate(no_of_episodes_sarsa, epsilon_sarsa, gamma_sarsa, alpha_sarsa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Qtable Results for *SARSA with ϵ-Greedy Behavior Policy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.162752</td>\n",
       "      <td>-0.831930</td>\n",
       "      <td>0.400986</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.526530</td>\n",
       "      <td>0.601335</td>\n",
       "      <td>0.040840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         00        01        02        03  04  05  06  07  08  09  ...  90  \\\n",
       "D  0.162752 -0.831930  0.400986 -0.300000   0   0   0   0   0   0  ...   0   \n",
       "U  0.000000  0.000000  0.000000  0.000000   0   0   0   0   0   0  ...   0   \n",
       "L  0.000000  0.052954  0.000000  0.222174   0   0   0   0   0   0  ...   0   \n",
       "R  0.526530  0.601335  0.040840  0.000000   0   0   0   0   0   0  ...   0   \n",
       "\n",
       "   91  92  93  94  95  96  97  98  99  \n",
       "D   0   0   0   0   0   0   0   0   0  \n",
       "U   0   0   0   0   0   0   0   0   0  \n",
       "L   0   0   0   0   0   0   0   0   0  \n",
       "R   0   0   0   0   0   0   0   0   0  \n",
       "\n",
       "[4 rows x 100 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa.fetchQtable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qlearning with an ϵ-greedy behavior policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Algorithm Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qlearning_sim:\n",
    "    '''\n",
    "    ** Please remember to reset Q and Returns table after simulation\n",
    "\n",
    "        Functions:\n",
    "        # Fetches Qtable ->                            .fetchQtable()    \n",
    "        # Run simulation ->                            .simulate(no_of_episodes, epsilon, gamma)\n",
    "        # Resets Qtable  ->                            .resettable()      \n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # Create Q table upon creating class\n",
    "        self.Qtable_qlearning = create_qtable(self.env.rows, self.env.cols, action_space)\n",
    "        \n",
    "    # Returns Qtable    \n",
    "    def fetchQtable(self):\n",
    "        return self.Qtable_qlearning\n",
    "    \n",
    "    # Resets Qtable by creating an empty table\n",
    "    def resettable(self):\n",
    "        self.Qtable_qlearning = createQtable()\n",
    "    \n",
    "    # Run Qlearning simulation\n",
    "            # There will be a sub step simulation within the overarching simulation - for looking ahead and updating Qtable\n",
    "    def simulate(self, no_of_episodes, epsilon, gamma, alpha):        \n",
    "        for i in range(no_of_episodes):    \n",
    "            # reset environment to start state\n",
    "            self.env.reset()  \n",
    "\n",
    "            # Initiialise main simulation state\n",
    "            main_step_state = self.env.current_state()\n",
    "\n",
    "            # Loop for main simulation\n",
    "            while self.env.is_terminal(self.env.current_state()) == False:\n",
    "                # Assign state so that it can be referenced again later after sub step simulation is conducted\n",
    "                main_step_state = self.env.current_state()\n",
    "\n",
    "                # Loop for sub simulation - Looking ahead until agent reaches terminal state \n",
    "                while self.env.is_terminal(self.env.current_state()) == False:\n",
    "                    sub_step_state = self.env.current_state()\n",
    "                    state_formatted = str(sub_step_state[0]) + str(sub_step_state[1])\n",
    "                    # Choose action in sub step simulation\n",
    "                    sub_step_action = epsilon_soft(self.Qtable_qlearning, self.env, epsilon, sub_step_state)\n",
    "                    \n",
    "                    # Taking action - to observe next state, action and rewar\n",
    "                    self.env.move(sub_step_action)\n",
    "\n",
    "                    # Retrieve reward for taking specific action\n",
    "                    reward = self.env.get_rewards()\n",
    "                    # Retrieve new state\n",
    "                    new_sub_step_state = self.env.current_state()\n",
    "                    new_state_formatted = str(new_sub_step_state[0]) + str(new_sub_step_state[1])\n",
    "\n",
    "                    # print(\"Sub State: {}\\nSub Action: {}\\n\\n\".format(sub_step_state, sub_step_action)) ## Test print\n",
    "\n",
    "                    # Check if new state is a terminal state: If not, then retrieve action for new state\n",
    "                    if new_sub_step_state in self.env.actions:\n",
    "                        new_sub_step_action = epsilon_soft(self.Qtable_qlearning, self.env, epsilon, new_sub_step_state)\n",
    "                        \n",
    "                        # Update Q(s1, a1) in direction of Q(s2, a2) where Q(s2, a2) is max qvalue at state s2\n",
    "                        self.Qtable_qlearning.loc[sub_step_action, state_formatted] += (alpha * (reward + (gamma * self.Qtable_qlearning[new_state_formatted].max()) - self.Qtable_qlearning.at[sub_step_action, state_formatted]))\n",
    "\n",
    "                    # If state is a terminal state, then update Q(s1, a1) with same equation, but Q(s2, a2) = 0 (since terminal state)\n",
    "                    else:\n",
    "                        self.Qtable_qlearning.loc[sub_step_action, state_formatted] += (alpha * (reward + (gamma * 0) - self.Qtable_qlearning.at[sub_step_action, state_formatted]))\n",
    "                        break # break out of sub simulation since it has reached terminal state\n",
    "\n",
    "                # Since we now want to revert back to the main simulation, we will need to reassign agent back to the main state  \n",
    "                self.env.set_state(main_step_state)\n",
    "                # Choose action based on policy with new Q values updated by sub simulation\n",
    "                main_step_action = epsilon_soft(self.Qtable_qlearning, self.env, epsilon, main_step_state)\n",
    "                # Move agent\n",
    "                self.env.move(main_step_action) \n",
    "\n",
    "                # print(\"Main state: {}\\nMain Action: {}\".format(main_step_state, main_step_action)) ## Test print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Running Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_qlearning = 0.1 # Epsilon greedy probability \n",
    "gamma_qlearning = 0.9 # Rewards discount rate gamma\n",
    "alpha_qlearning = 0.3 # Learning rate of agent\n",
    "no_of_episodes_qlearning = 100 # Number of episodes to be executed in simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Creating instance of class & running simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of class with environment 1 (4x4 grid)\n",
    "qlearning = qlearning_sim(env1)\n",
    "\n",
    "# Run Qlearning simulation\n",
    "qlearning.simulate(no_of_episodes_qlearning, epsilon_qlearning, gamma_qlearning, alpha_qlearning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Qtable Results for *Qlearning with ϵ-Greedy Behavior Policy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.272762</td>\n",
       "      <td>-0.971752</td>\n",
       "      <td>0.729000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523072</td>\n",
       "      <td>0.582317</td>\n",
       "      <td>0.654572</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.590490</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.563733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         00        01        02        03  04  05  06  07  08  09  ...  90  \\\n",
       "D  0.272762 -0.971752  0.729000 -0.300000   0   0   0   0   0   0  ...   0   \n",
       "U  0.000000  0.000000  0.000000  0.000000   0   0   0   0   0   0  ...   0   \n",
       "L  0.000000  0.523072  0.582317  0.654572   0   0   0   0   0   0  ...   0   \n",
       "R  0.590490  0.656100  0.563733  0.000000   0   0   0   0   0   0  ...   0   \n",
       "\n",
       "   91  92  93  94  95  96  97  98  99  \n",
       "D   0   0   0   0   0   0   0   0   0  \n",
       "U   0   0   0   0   0   0   0   0   0  \n",
       "L   0   0   0   0   0   0   0   0   0  \n",
       "R   0   0   0   0   0   0   0   0   0  \n",
       "\n",
       "[4 rows x 100 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlearning.fetchQtable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "00073238948e3e314995d1c989df02e5591cbd633de70172eca577de89016890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
